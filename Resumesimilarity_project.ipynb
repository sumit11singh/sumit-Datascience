{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EWzRTe4RhDb",
        "outputId": "d8b2142e-8887-4790-d86c-57d6941e1dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=1206cc6eb39742c2749f7f9c8ca50509d4e3fe0ae1f881f6f214f8e603f5cc04\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ],
      "source": [
        "! pip install docx2txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx2txt"
      ],
      "metadata": {
        "id": "jHOedUvuRvz2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_description=docx2txt.process('/content/data-scientist-resume-example.pdf')\n",
        "resume=docx2txt.process('/content/JASMEET_SINGH_Data Analyst.pdf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "4Mx3tn2fR9R7",
        "outputId": "6339af55-b789-4308-bb64-81e4e4623d33"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bc91bcbbbba1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjob_description\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocx2txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/data-scientist-resume-example.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocx2txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/JASMEET_SINGH_Data Analyst.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docx2txt/docx2txt.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(docx, img_dir)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# unzip the docx in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mzipf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mfilelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume=docx2txt.process('/content/JASMEET_SINGH_Data Analyst.docx')\n",
        "job_description=docx2txt.process('/content/data-scientist-resume-example.docx')"
      ],
      "metadata": {
        "id": "cH9hm7k9S7PZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resume)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTWjAkE5YcrX",
        "outputId": "37529de7-06d1-4cf3-c546-ca5db452f1e7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JASMEET SINGH\n",
            "\n",
            "DATA ANALYST\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "9216688777\n",
            "\n",
            "\n",
            "\n",
            "Singh.jasmit013@gmail.com Chandigarh\n",
            "\n",
            "\n",
            "B Tech\n",
            "\n",
            "\tInformation Technology\tAug '05 - Jun '11 NIET | Greater Noida,U.P\n",
            "\n",
            "Uttar Pradesh Technical University\n",
            "\n",
            "\n",
            "\n",
            "linkedin.com/in/jasmeet-singh-0a\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "8 + years experience of Telecomm professional in Optical Fiber\n",
            "\n",
            "\n",
            "PG diploma in Data science and machine learning\n",
            "\n",
            "Data Trained | Noida NOV 2021 - NOV 2022\n",
            "\n",
            "\n",
            "Nov '21 - Nov '22\n",
            "\n",
            "\n",
            "\n",
            "Responsible for the installation activities/execution is in line with the installation planning.\n",
            "\n",
            "Installation of all equipment as like FDP, Splitters, FAT & Duct, Optic Fiber Cable.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CAREER OBJECTIVE\n",
            "\n",
            "Innovative and scientifically rigorous with a significant data science internship\n",
            "\n",
            "experience to bring to the table. With a team-oriented attitude, I am eager to\n",
            "\n",
            "contribute my abilities in quantitative modeling and experimentation to\n",
            "\n",
            "enhance the experience of Pinterest users around the world.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SKILLS\n",
            "\n",
            "Data Mining Data Manipulation Data Scientist Perform analysis Statistical Modeling Modeling Techniques , perform Data Analysis on Tableau & power BI software Data Visualization Techniques\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "PROJECTS\n",
            "\n",
            "HR Attrition\n",
            "\n",
            "Tech Stack: Python, Jupyter Notebook\n",
            "\n",
            "Objective: Build a predictive model for detecting defaulter and non- defaulter users.\n",
            "\n",
            "\n",
            "\n",
            "CUSTOMER CHURN\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tech Stack: Python, Jupyter Notebook\n",
            "\n",
            "Objective: Predict the probability of each variation belonging to nine classes.\n",
            "\n",
            "Solution: Did EDA, treated data imbalance , built multiple models & did hyperparameter tuning for best performing models in each category, treated outliers using percentile approach without removing the data\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "WORK EXPERIENCE\n",
            "\n",
            "\tWireline Trainer\tNov '22 - Present Airtel India | Chandigarh\n",
            "\n",
            " Conducted Training the Engineers with classroom and virtual trainings.\n",
            "\n",
            "\t\t\tPerformed training sessions for 10+ new recruits as part of improving their skills.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\tData Scientist Intern\tApr '22 - Oct '22 Flip Robo Technologies | Bangalore\n",
            "\n",
            "April 2022 - October 2022 Bangalore\n",
            "\n",
            " Performed Extensively on web scrapped images, NLP, data analysis, Clustering Algorithms.\n",
            "\n",
            " Performed Extensively on web scrapped images, NLP, data analysis, Clustering Algorithms.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\tSenior Engineer Project\tJun '18 - Oct '22 Reliance Communications | Bikaner\n",
            "\n",
            " Identifying the Potential prospective OFC routes for Laying of Optical Fiber issued by Network Planning Team Roll Out of FTTX Network.\n",
            "\n",
            " I was responsible to give the status of physical progress (Day to Day reporting.\n",
            "\n",
            " Administered the technical and commercial verification of revenue recognition with actual work executed so that all extra works are capture.\n",
            "\n",
            "\n",
            "\n",
            "HOBBIES\n",
            "\n",
            "Playing Cricket Reading Books\n",
            "\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(job_description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOrCX9OzY5S2",
        "outputId": "efec27d7-ffb3-4643-d3ff-7f77c43a0233"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KANDACE  LOUDOR\n",
            "\n",
            "DATA SCIENTIST\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CONTACT\n",
            "\n",
            "kloudor@email.com\n",
            "\n",
            "(123) 456-7890\n",
            "\n",
            "Mount Laurel, NJ\n",
            "\n",
            "LinkedIn Github\n",
            "\n",
            "\n",
            "\n",
            "EDUCATION\n",
            "\n",
            "B.S.\n",
            "\n",
            "Statistics Rutgers University September 2011 - April 2015\n",
            "\n",
            "New Brunswick, NJ\n",
            "\n",
            "\n",
            "\n",
            "SKILLS\n",
            "\n",
            "Python (NumPy, Pandas, Scikit-learn, Keras, Flask) SQL (MySQL, Postgres)\n",
            "\n",
            "Git Time Series Forecasting Productionizing Models Recommendation Engines Customer Segmentation\n",
            "\n",
            "AWS\n",
            "\n",
            "\n",
            "WORK EXPERIENCE\n",
            "\n",
            "Data Scientist\n",
            "\n",
            "Grubhub\n",
            "\n",
            "June 2018 - current / Princeton, NJ\n",
            "\n",
            "Deployed a recommendation engine to production to conditionally recommend other menu items based on past order history, increasing average order size by 7%\n",
            "\n",
            "Implemented various time series forecasting techniques to predict surge in orders, lowering customer wait by 10 minutes\n",
            "\n",
            "Designed a model in a pilot to increase incentives for drivers during peak hours, increasing driver availability by 22%\n",
            "\n",
            "Led a team of 3 data scientist to model the ordering process 5 unique ways, reported results, and made recommendations to increase order output by 9%\n",
            "\n",
            "\n",
            "\n",
            "Data Scientist\n",
            "\n",
            "Spectrix Analytical Services\n",
            "\n",
            "March 2016 - June 2018 / Princeton, NJ\n",
            "\n",
            "Built a customer attrition random forest model that improved monthly retention by 12 basis points for clients likely to opt-out by providing relevant product features for them\n",
            "\n",
            "Coordinated with the product and marketing teams to determine what kind of client interactions resulted in maximized service opt-ins, increasing conversions by 18%\n",
            "\n",
            "Partnered with product team to create a production recommendation engine in Python that improved the length on- page for users with $225K in incremental annual revenue\n",
            "\n",
            "Compiled and analyzed data surrounding the prototypes for a prosthesis, which saved over $1M in its creation\n",
            "\n",
            "\n",
            "\n",
            "Entry-Level Data Analyst\n",
            "\n",
            "Avenica\n",
            "\n",
            "April 2015 - March 2016 / Mount Laurel, NJ\n",
            "\n",
            "Collaborated with product managers to perform cohort analysis that identified an opportunity to reduce pricing by 21% for a segment of users to boost yearly revenue by $560,000 Constructed operational reporting in Tableau to improve\n",
            "\n",
            "scheduling contractors, saving $90,000 in the annual budget\n",
            "\n",
            "Implemented a long-term pricing experiment that improved customer lifetime value by 23%\n",
            "\n",
            "Ran, submitted, and reported on monthly client enrollments, services opted in for, and the employees assigned to clients\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content=[job_description,resume]"
      ],
      "metadata": {
        "id": "nbQE50jnY8c1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "cl4WoSsYZPIl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv=CountVectorizer()"
      ],
      "metadata": {
        "id": "vdlUeQZpZqdr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrix=cv.fit_transform(content)"
      ],
      "metadata": {
        "id": "GuT-zxsPZt7y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "aMiAj7N_aLop"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simlarity_score=cosine_similarity(metrix)"
      ],
      "metadata": {
        "id": "IGcizhtOaiTw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(simlarity_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7iwnSA5a3Z3",
        "outputId": "13e245cd-2711-442c-f183-7460c1a3bcce"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.        0.4994631]\n",
            " [0.4994631 1.       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('resume matches by:'+str(simlarity_score[1][0]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGsDZRmgbN-g",
        "outputId": "b9af59c7-ef5b-4a01-bf4a-f94e2d5b9c7d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resume matches by:49.94630998516077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPAXVGDzbjTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eohd85OffJHc"
      }
    }
  ]
}